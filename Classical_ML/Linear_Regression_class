import numpy as np

class Smoh_Linear_Regression:
    "for normal form just call (lr_fit) for the Gradient descent one call (lr_gd_fit)"

    @staticmethod
    def fit(X, y):
        "give a X(it is a numpy array) as the features and y as the actual values! It returns the optimal values of Theta"
        "First output is the best theta value for the bias column and all others are for the features!"
        "Also no need to add the bias column it adds it automatically"
        n = X.shape[0]
        # adding bias column!
        X = np.c_[np.ones(n), X]
        theta = np.dot(np.linalg.inv(np.dot(X.T, X)), X.T).dot(y)

        return theta

    @staticmethod
    def predict(theta, X):
        "Give the theta too which u calculated from the fit method"
        "for prediction give a dataframe with correct dimension and no need to add bias term!"
        n = X.shape[0]  
        X = np.c_[np.ones(n), X]
        return np.dot(X, theta)

    @staticmethod
    def gd_fit(X, y, learning_rate=0.01, epochs=10000):
        n, m = X.shape
        y = np.array(y).reshape(-1, 1)
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X = np.c_[np.ones(n), X]
        theta = np.random.rand(X.shape[1], 1)
        for i in range(epochs):
            gradient = (1/n) * X.T.dot(X.dot(theta) - y)
            theta -= learning_rate * gradient  
            if i % 1000 == 0:
                print(f"Epoch {i}: Theta = {theta.flatten()}")
        return theta.flatten()

    @staticmethod
    def mse(y_true, y_pred):
        return np.mean((y_true - y_pred) ** 2)
