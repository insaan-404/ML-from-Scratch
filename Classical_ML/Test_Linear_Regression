"This is completly Ai generated!!!!"
"Just for the sake of testing!"


import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt



# Set random seed for reproducibility
np.random.seed(42)

# Generate synthetic regression data
X, y = make_regression(n_samples=1000, n_features=5, noise=0.5, random_state=42)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("=== Testing Normal Form Linear Regression ===")
# Test Smoh_Linear_Regression normal form solution
smoh_lr = Smoh_Linear_Regression()
theta = smoh_lr.fit(X_train, y_train)
y_pred_smoh = smoh_lr.predict(theta, X_test)
mse_smoh = smoh_lr.mse(y_test, y_pred_smoh)
print(f"Smoh Linear Regression MSE: {mse_smoh:.4f}")

# Test scikit-learn LinearRegression
sklearn_lr = LinearRegression()
sklearn_lr.fit(X_train, y_train)
y_pred_sklearn = sklearn_lr.predict(X_test)
mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)
print(f"Scikit-learn Linear Regression MSE: {mse_sklearn:.4f}")

# Test Smoh_Linear_Regression with gradient descent
print("\n=== Testing Gradient Descent Linear Regression ===")
theta_gd = smoh_lr.gd_fit(X_train, y_train, learning_rate=0.01, epochs=5000)
# Since gd_fit normalizes the data, we need to normalize the test data too
X_test_normalized = (X_test - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)
y_pred_smoh_gd = smoh_lr.predict(theta_gd, X_test_normalized)
mse_smoh_gd = smoh_lr.mse(y_test, y_pred_smoh_gd)
print(f"Smoh Gradient Descent Linear Regression MSE: {mse_smoh_gd:.4f}")

# Compare coefficients
print("\n=== Coefficient Comparison ===")
print(f"Smoh Normal Form Coefficients: {theta}")
# Need to reshape sklearn coefficients to include intercept at the beginning for comparison
sklearn_full_coefs = np.concatenate(([sklearn_lr.intercept_], sklearn_lr.coef_))
print(f"Scikit-learn Coefficients: {sklearn_full_coefs}")
print(f"Coefficient difference (L2 norm): {np.linalg.norm(theta - sklearn_full_coefs):.6f}")

# Visualize predictions comparison for the first feature
plt.figure(figsize=(12, 6))

# Sort test data by first feature for cleaner visualization
sort_idx = np.argsort(X_test[:, 0])
X_test_sorted = X_test[sort_idx]
y_test_sorted = y_test[sort_idx]
y_pred_sklearn_sorted = y_pred_sklearn[sort_idx]
y_pred_smoh_sorted = y_pred_smoh[sort_idx]

plt.subplot(1, 2, 1)
plt.scatter(X_test_sorted[:, 0], y_test_sorted, s=10, alpha=0.5, label='True Values')
plt.plot(X_test_sorted[:, 0], y_pred_sklearn_sorted, 'r-', label='Scikit-learn')
plt.plot(X_test_sorted[:, 0], y_pred_smoh_sorted, 'g--', label='Smoh Normal')
plt.xlabel('Feature 1')
plt.ylabel('Target')
plt.title('Predictions Comparison')
plt.legend()

# Plot residuals
plt.subplot(1, 2, 2)
plt.scatter(y_pred_sklearn_sorted, y_pred_sklearn_sorted - y_test_sorted, alpha=0.5, label='Scikit-learn Residuals')
plt.scatter(y_pred_smoh_sorted, y_pred_smoh_sorted - y_test_sorted, alpha=0.5, label='Smoh Residuals')
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.legend()

plt.tight_layout()
plt.savefig('linear_regression_comparison.png')
plt.show()

print("\nDone! Test completed.")
